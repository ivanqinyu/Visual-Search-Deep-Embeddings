{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "honey-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from  PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "willing-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "under-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "challenging-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "novel-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101 = models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "small-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152 = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-interval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bulgarian-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the model.\n",
    "resnet = resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raised-partnership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "important-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stopped-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\richa\\Desktop\\projects\\Bristol uni porjects\\Univecity projects\\Applied Data Science\\uob_image_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "turkish-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_images_from_folder(folder, end, as_tensor = False):\n",
    "\n",
    "#     images = []\n",
    "#     files = folders = 0\n",
    "#     count= 0\n",
    "#     dirnames = [] \n",
    "\n",
    "#     num_img = []\n",
    "\n",
    "#     for _, dirnames, filenames in os.walk(folder):\n",
    "\n",
    " \n",
    "\n",
    "#         if dirnames != []:\n",
    "#             subfolders = dirnames\n",
    "#         current_path = os.path.join(folder, subfolders[count])\n",
    "#         num_img.append(len(os.listdir(current_path)))\n",
    "#         for filename in os.listdir(current_path):\n",
    "#             img = cv2.imread(os.path.join(current_path, filename))\n",
    "#             if img is not None:\n",
    "#                 if as_tensor:\n",
    "#                     img = torch.from_numpy(img * 1.0)\n",
    "#                     # img = torch.from_numpy(img).float()\n",
    "#                     images.append(img)\n",
    "#                 else: \n",
    "#                     img = Image.fromarray(img)\n",
    "# #                     print(type(img))\n",
    "#                     images.append(img)\n",
    "\n",
    " \n",
    "\n",
    "#         count += 1\n",
    "        \n",
    "#         if count % 10 == 0:\n",
    "#             print('number of folder done =', count, 'total number of images so far =', len(images))\n",
    "            \n",
    "#         if count  == end:\n",
    "#             return images, num_img\n",
    "#         # only do 300 out of the 1500 folders...is too big otherwise\n",
    "#         # if count == 100:\n",
    "#         #     return images\n",
    "#     return images, num_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "heavy-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, end, as_tensor = False):\n",
    "\n",
    "    images = []\n",
    "    files = folders = 0\n",
    "    count= 0\n",
    "    dirnames = [] \n",
    "\n",
    "    num_img = []\n",
    "\n",
    "    for _, dirnames, filenames in os.walk(folder):\n",
    "\n",
    " \n",
    "\n",
    "        if dirnames != []:\n",
    "            subfolders = dirnames\n",
    "        current_path = os.path.join(folder, subfolders[count])\n",
    "        num_img.append(len(os.listdir(current_path)))\n",
    "        for filename in os.listdir(current_path):\n",
    "            img = Image.open(os.path.join(current_path, filename))\n",
    "            if img is not None:\n",
    "                if as_tensor:\n",
    "                    img = torch.from_numpy(img * 1.0)\n",
    "                    images.append(img)\n",
    "                else: \n",
    "                    images.append(img)\n",
    "\n",
    " \n",
    "\n",
    "        count += 1\n",
    "        \n",
    "        if count % 10 == 0:\n",
    "            print('number of folder done =', count, 'total number of images so far =', len(images))\n",
    "            \n",
    "        if count  == end:\n",
    "            return images, num_img\n",
    "        # only do 300 out of the 1500 folders...is too big otherwise\n",
    "        # if count == 100:\n",
    "        #     return images\n",
    "    return images, num_img\n",
    "\n",
    "\n",
    "def show_image(PIL_img):\n",
    "    PIL_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, end, as_tensor = False):\n",
    "        \n",
    "    images = []\n",
    "    files = folders = 0\n",
    "    # count denotes the number of folders to search through\n",
    "    count= 0\n",
    "    dirnames = [] \n",
    "\n",
    " \n",
    "\n",
    "    for _, dirnames, filenames in os.walk(folder):\n",
    "\n",
    " \n",
    "\n",
    "        if dirnames != []:\n",
    "            subfolders = dirnames\n",
    "        current_path = os.path.join(folder, subfolders[count])  \n",
    "        for filename in os.listdir(current_path):\n",
    "            img = Image.open(os.path.join(current_path, filename))\n",
    "            if img is not None:\n",
    "                if as_tensor:\n",
    "                    img = torch.from_numpy(img * 1.0)\n",
    "                    # img = torch.from_numpy(img).float()\n",
    "                    images.append(img)\n",
    "                else: \n",
    "                    images.append(img)\n",
    "\n",
    " \n",
    "\n",
    "        count += 1\n",
    "        \n",
    "        if count  == end:\n",
    "            return images\n",
    "        \n",
    "    return images\n",
    "\n",
    "\n",
    "def show_image(PIL_img):\n",
    "    PIL_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "occupational-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, num_img_list = load_images_from_folder(path, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pursuant-thumb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "traditional-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 4]\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(num_img_list), print(sum(num_img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "generic-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_input_tensor = []\n",
    "list_input_batch = []\n",
    "num_img = sum(num_img_list)\n",
    "for item in range(num_img - 1):\n",
    "    input_tensor = preprocess(dataset[item])\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    list_input_tensor.append(input_tensor)\n",
    "    list_input_batch.append(input_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "mobile-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "absent-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = torch.nn.Linear(num_ftrs, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unique-transformation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7197,  0.4163, -0.1825]], device='cuda:0')\n",
      "tensor([[-0.7517,  0.4607, -0.1521]], device='cuda:0')\n",
      "tensor([[-0.7484,  0.4555, -0.1553]], device='cuda:0')\n",
      "tensor([[-0.7220,  0.4278, -0.1583]], device='cuda:0')\n",
      "tensor([[-0.7416,  0.4681, -0.1596]], device='cuda:0')\n",
      "tensor([[-0.7488,  0.4658, -0.1630]], device='cuda:0')\n",
      "tensor([[-0.7231,  0.4040, -0.1827]], device='cuda:0')\n",
      "tensor([[-0.7260,  0.4446, -0.1814]], device='cuda:0')\n",
      "tensor([[-0.7261,  0.4534, -0.1884]], device='cuda:0')\n",
      "tensor([[-0.6819,  0.4259, -0.1550]], device='cuda:0')\n",
      "tensor([[-0.7501,  0.4809, -0.1387]], device='cuda:0')\n",
      "tensor([[-0.6775,  0.4692, -0.1547]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_input_batch)):\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = list_input_batch[i].to('cuda')\n",
    "        resnet.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = resnet(input_batch)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "selective-carnival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29384795, -0.35663211,  0.02612396],\n",
       "       [-0.30435032, -0.3663305 ,  0.02636867],\n",
       "       [-0.27115431, -0.37009948,  0.06807595],\n",
       "       [-0.29218262, -0.38294515,  0.05986913],\n",
       "       [-0.26686826, -0.36159807,  0.07712689],\n",
       "       [-0.27379483, -0.35311711,  0.04263894],\n",
       "       [-0.26608825, -0.34815207,  0.03915934],\n",
       "       [-0.27748123, -0.3415657 ,  0.01566445],\n",
       "       [-0.30066267, -0.35829026,  0.05926574],\n",
       "       [-0.25781876, -0.3173559 ,  0.08615623],\n",
       "       [-0.32359609, -0.35515097,  0.06267235],\n",
       "       [-0.27371293, -0.36206087,  0.06394663]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_array = []\n",
    "for i in range(len(list_input_batch)):\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = list_input_batch[i].to('cuda')\n",
    "        resnet.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = resnet(input_batch)\n",
    "        cup_tensor = output.cpu()\n",
    "        \n",
    "    output_array.append(cup_tensor[0].tolist())\n",
    "#     output_array.append(np.asarray(cup_tensor[0]))\n",
    "#     print(np.asarray(cup_tensor[0]))\n",
    "output_array = np.asarray(output_array)\n",
    "display(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "official-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "from scipy.interpolate import *\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "convinced-engine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ca1bea80fd4cf5921f87c016e4869f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab\n",
    "import numpy \n",
    "import ipympl\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "img_counter = 0\n",
    "for n in num_img_list:\n",
    "    r = lambda: random.randint(0,255)\n",
    "    random_color = '#%02X%02X%02X' % (r(),r(),r())\n",
    "    ax.scatter(output_array[img_counter:img_counter + n,0], output_array[img_counter:img_counter + n,1], output_array[img_counter:img_counter + n,2],color=random_color,s=20)\n",
    "    img_counter += n\n",
    "plt.show()  \n",
    "\n",
    "ax.set_xlabel('X Axes')\n",
    "ax.set_ylabel('Y Axes')\n",
    "ax.set_zlabel('Frame Axes')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# length = data.shape[0]\n",
    "# width = data.shape[1]\n",
    "# x, y = np.meshgrid(np.arange(length), np.arange(width))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "# ax.plot_surface(x, y, data)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "infrared-characteristic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b5dfb71bde40678ffd1571c1ceba32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "img_counter = 0\n",
    "for n in num_img_list:\n",
    "    r = lambda: random.randint(0,255)\n",
    "    random_color = '#%02X%02X%02X' % (r(),r(),r())\n",
    "    plt.plot(output_array[img_counter:img_counter + n,0], output_array[img_counter:img_counter + n,1], 'o',color=random_color)\n",
    "    img_counter += n\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ranging-soccer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d23a42430d4e489b6ce2857fb99ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "img_counter = 0\n",
    "for n in num_img_list:\n",
    "    r = lambda: random.randint(0,255)\n",
    "    random_color = '#%02X%02X%02X' % (r(),r(),r())\n",
    "    plt.plot(output_array[img_counter:img_counter + n,0], output_array[img_counter:img_counter + n,2], 'o',color=random_color)\n",
    "    img_counter += n\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "recent-seattle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d501e408bd9341ebaaae98569d696a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "img_counter = 0\n",
    "for n in num_img_list:\n",
    "    r = lambda: random.randint(0,255)\n",
    "    random_color = '#%02X%02X%02X' % (r(),r(),r())\n",
    "    plt.plot(output_array[img_counter:img_counter + n,1], output_array[img_counter:img_counter + n,2], 'o',color=random_color)\n",
    "    img_counter += n\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-canberra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-behavior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-sight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-worker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-logging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-corruption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-stevens",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-nevada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-synthetic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-romance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-metabolism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "brilliant-narrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57982e59e133405d83285c7176e0f4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(output_array[:4,0], output_array[:4,1], 'o',color='r')\n",
    "plt.plot(output_array[4:,0], output_array[4:,1], 'o',color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "devoted-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee111be37d24b25821ae1c170c3cde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(output_array[:4,0], output_array[:4,2], 'o',color='r')\n",
    "\n",
    "plt.plot(output_array[4:,0], output_array[4:,2], 'o',color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "female-intranet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f87b753467a44aa91788fd19181195f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(output_array[:4,1], output_array[:4,2], 'o',color='r')\n",
    "plt.plot(output_array[4:,1], output_array[4:,2], 'o',color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "preliminary-sampling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97223af4ec524d5db36f6694366cdecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab\n",
    "import numpy \n",
    "import ipympl\n",
    "\n",
    "# %matplotlib widget\n",
    "# %matplotlib notebook\n",
    "\n",
    "# x = [item[0] for item in output_array]\n",
    "# y = [item[1] for item in output_array]\n",
    "# z = [item[2] for item in output_array]\n",
    "# # output_array[:,0], output_array[:,1], output_array[:,2]\n",
    "\n",
    "# output_array_1 = numpy.array(output_array)\n",
    "# print(output_array_1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.scatter(output_array[:4,0], output_array[:4,1], output_array[:4,2],color='r',s=3)\n",
    "ax.scatter(output_array[4:8,0], output_array[4:8,1], output_array[4:8,2],color='y',s=3)\n",
    "ax.scatter(output_array[8:,0], output_array[8:,1], output_array[8:,2],color='b',s=3)\n",
    "ax.set_xlabel('X Axes')\n",
    "ax.set_ylabel('Y Axes')\n",
    "ax.set_zlabel('Frame Axes')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# length = data.shape[0]\n",
    "# width = data.shape[1]\n",
    "# x, y = np.meshgrid(np.arange(length), np.arange(width))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "# ax.plot_surface(x, y, data)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-weight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
